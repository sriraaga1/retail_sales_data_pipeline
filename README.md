# Retail Sales Data Pipeline  
**End-to-End ETL Project (Python + SQLite + DuckDB + Parquet + Matplotlib)**  
Cleaned, validated, stored, and analyzed retail sales data with SQL queries and visual dashboards.

---

## Project Overview  
This project demonstrates a **complete data-engineering workflow** â€” from raw CSV extraction to data validation, transformation, storage, and visualization.  
It simulates a **mini data pipeline** that can later be scaled to cloud or production systems.

It covers:  
âœ… Data cleaning & transformation  
âœ… Validation checks on key columns  
âœ… Loading into SQLite database  
âœ… Generating reports & charts  
âœ… Querying Parquet data using DuckDB (for analytics at scale)

---

## ğŸ§© Tech Stack  

| Layer | Tools Used |
|--------|-------------|
| Programming | Python (Pandas, NumPy) |
| Storage | SQLite, Parquet |
| Query Engine | DuckDB |
| Visualization | Matplotlib |
| ETL Process | Custom Python Scripts |
| Version Control | Git + GitHub |

---

## ğŸ§  Workflow Summary  

1. **Extract** â†’ Read raw CSV from `/data/retail_sales.csv`  
2. **Transform** â†’ Clean columns, handle missing values, compute monthly/yearly metrics  
3. **Validate** â†’ Apply data-quality rules (e.g., unique transaction_id, non-negative revenue)  
4. **Load** â†’ Store the clean data in SQLite (`retail_sales.db`)  
5. **Visualize** â†’ Generate analytical charts (Top Categories, Monthly Revenue, Top Items)  
6. **Analyze** â†’ Query Parquet data with DuckDB and export results as CSV  

---

## ğŸ§¾ Folder Structure  

retail_sales_data_pipeline/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ retail_sales.csv
â”‚ â”œâ”€â”€ cleaned_sales.csv
â”‚ â””â”€â”€ cleaned_sales.parquet
â”‚
â”œâ”€â”€ charts/
â”‚ â”œâ”€â”€ revenue_by_category.png
â”‚ â”œâ”€â”€ monthly_revenue_by_year.png
â”‚ â””â”€â”€ top_items_by_revenue.png
â”‚
â”œâ”€â”€ duckdb_outputs/
â”‚ â”œâ”€â”€ top_categories.csv
â”‚ â””â”€â”€ monthly_revenue.csv
â”‚
â”œâ”€â”€ extract.py
â”œâ”€â”€ clean_transform.py
â”œâ”€â”€ validate.py
â”œâ”€â”€ load_to_sqlite.py
â”œâ”€â”€ make_charts.py
â”œâ”€â”€ duckdb_queries.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

---

---

## ğŸš€ How to Run

### ğŸ§© Step 1 â€” Setup Environment
```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate      # (Windows: .venv\Scripts\activate)

# Install dependencies
pip install -r requirements.txt

âš™ï¸ Step 2 â€” Run ETL Pipeline

python extract.py
python clean_transform.py
python validate.py             # âœ… should print "Data validation passed"
python load_to_sqlite.py
python make_charts.py
python duckdb_queries.py



ğŸ“Š 3. Outputs
| Step         | Output                  | Location          |
| ------------ | ----------------------- | ----------------- |
| Cleaned Data | `cleaned_sales.csv`     | `/data`           |
| SQLite DB    | `retail_sales.db`       | root              |
| Charts       | `.png` files            | `/charts`         |
| Parquet File | `cleaned_sales.parquet` | `/data`           |
| SQL Reports  | `.csv` summaries        | `/duckdb_outputs` |


âœ… Data-Validation Rules
| Rule                 | Description                                     |
| -------------------- | ----------------------------------------------- |
| Required Columns     | `transaction_id`, `transaction_date`, `revenue` |
| No Missing Values    | No nulls in critical columns                    |
| Non-Negative Revenue | All revenue â‰¥ 0                                 |
| Unique IDs           | Each `transaction_id` is unique                 |


ğŸ“ˆ Example Visuals
| Chart                       | Description                                   |
| --------------------------- | --------------------------------------------- |
| **Revenue by Category**     | Top 10 categories contributing to total sales |
| **Monthly Revenue by Year** | Trend visualization across years              |
| **Top Items by Revenue**    | Highest-selling products overall              |


ğŸ§° Skills Demonstrated

Python for Data Engineering
Pandas for Cleaning & Transformation
SQL + SQLite for Structured Data Storage
DuckDB for In-Memory Analytics
Parquet for Columnar Storage
Matplotlib for Visualization
Data Validation & Quality Checks
ETL Workflow Design
Git & GitHub for Version Control
