# Retail Sales Data Pipeline (Python + SQLite + DuckDB)

End-to-end project: **Extract â†’ Transform â†’ Validate â†’ Load â†’ Analyze â†’ Visualize**

**Tech Stack:** Python Â· Pandas Â· SQLite Â· Matplotlib Â· DuckDB Â· Parquet  
**Dataset:** ~12,000 retail transactions â†’ ~11,971 after cleaning

---

## ğŸ—‚ Project Structure
retail_sales_data_pipeline/
â”œâ”€ data/ # raw & cleaned data (CSV/Parquet)
â”œâ”€ duckdb_outputs/ # SQL outputs from DuckDB
â”œâ”€ charts/ # generated charts
â”œâ”€ extract.py # read raw CSV, preview data
â”œâ”€ clean_transform.py # clean + transform â†’ cleaned_sales.csv
â”œâ”€ validate.py # data validation (columns, duplicates, etc.)
â”œâ”€ load_to_sqlite.py # save cleaned data to retail_sales.db
â”œâ”€ make_charts.py # visualize key insights
â”œâ”€ duckdb_queries.py # run SQL on Parquet + export results
â””â”€ requirements.txt


---

## ğŸš€ How to Run

### 1ï¸âƒ£ Setup Environment
```bash
python -m venv .venv
source .venv/bin/activate        # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
2ï¸âƒ£ Extract â†’ Transform â†’ Validate

python extract.py
python clean_transform.py
python validate.py                 # âœ… should print â€œData validation passedâ€
3ï¸âƒ£ Load to SQLite
python load_to_sqlite.py

4ï¸âƒ£ Generate Charts
python make_charts.py

5ï¸âƒ£ Run DuckDB SQL Queries
python duckdb_queries.py


ğŸ“Š Outputs
Cleaned data â†’ data/cleaned_sales.csv
Parquet file â†’ data/cleaned_sales.parquet
SQLite database â†’ retail_sales.db (table sales_data)
Charts â†’ charts/*.png
SQL summaries â†’ duckdb_outputs/*.csv

âœ… Data Validation Rules
Required columns present: transaction_id, transaction_date, revenue
No null transaction dates
No negative revenue
Unique transaction_id


ğŸ’¡ Skills Demonstrated
Python | Pandas | SQL | SQLite | DuckDB | Data Cleaning | ETL | Parquet | Matplotlib | Data Visualization